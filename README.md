# Transformers from scratch with PyTorch

A PyTorch implementation of the work "Attention is All You Need" with some goals:

Achieved:
* An educational implementation (not a performant one)

Next steps:
* Reproduce the results from the original paper

## Requirements

* Python 3.10.12
* requirements.txt


## Resources

These were the main resources I used to understand and implement the model. 

Transformer Architecture:
* [Original Paper](https://arxiv.org/abs/1706.03762)
* Step-by-step guide into the architecture: [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
* Implementation from scratch (w/o PE + forward expansion): [Pytorch Transformers from Scratch (Attention is all you need)](https://www.youtube.com/watch?v=U0s0f995w14)

Positional Encoding mechanism:
* Understanding the mechanism: [Master Positional Encoding Part 1](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)
* Implementation: [Machine Learning Mastery](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)
